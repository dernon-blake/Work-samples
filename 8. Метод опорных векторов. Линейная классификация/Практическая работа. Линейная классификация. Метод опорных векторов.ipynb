{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIu2DPx5tWcg"
   },
   "source": [
    "## Практическая работа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEdLy24do28f"
   },
   "source": [
    "## Цели практической работы\n",
    "\n",
    "*  решить реальную задачу;\n",
    "*  потренироваться в обработке данных;\n",
    "*  обучить различные модели классификации, подобрать гиперпараметры и выбрать лучшую модель;\n",
    "*  добиться наилучшего качества в задаче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6oxDXgFpD_6"
   },
   "source": [
    "## Что входит в практическую работу\n",
    "\n",
    "*  исследование датасета и обработка данных (работа с пропущенными и ошибочными значениями);\n",
    "*  обучение различных моделей классификации с параметрами по умолчанию;\n",
    "*  подбор гиперпараметров моделей;\n",
    "*  смешивание моделей;\n",
    "*  оценка качества моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOZ8SueHpZGd"
   },
   "source": [
    "## Что оценивается\n",
    "\n",
    "*  Выполнены все этапы задания: код запускается, отрабатывает без ошибок; подробно и обоснованно написаны текстовые выводы, где это требуется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMr-wvh3pbrK"
   },
   "source": [
    "## Формат сдачи\n",
    "Выполните предложенные задания: впишите свой код (или, если требуется, текст) в ячейки после комментариев. \n",
    "\n",
    "*Комментарии — это текст, который начинается с символа #. Например: # ваш код здесь.*\n",
    "\n",
    "Сохраните изменения, используя опцию Save and Checkpoint из вкладки меню File или кнопку Save and Checkpoint на панели инструментов. Итоговый файл в формате .ipynb (файл Jupyter Notebook) загрузите в личный кабинет и отправьте на проверку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENiWYl_EtWcm"
   },
   "source": [
    "### 1. Загрузите тренировочные и тестовые датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "id": "_uf-KNwQtWcn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain = pd.read_csv(\"TrainData.csv\")\n",
    "TEST = pd.read_csv(\"TestData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaELM2gPtWcq"
   },
   "source": [
    "### 2. Изучите тренировочные и тестовые данные на наличие:\n",
    "- пропусков,\n",
    "- ошибочных значений.\n",
    "\n",
    "Обработайте пропуски и ошибочные значения способом, выбранным по своему усмотрению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "id": "sqevAjp-tWcr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1           0\n",
      "f2          75\n",
      "f3           0\n",
      "f4           0\n",
      "f5           0\n",
      "f6           0\n",
      "f7        1875\n",
      "f8           0\n",
      "f9           0\n",
      "f10          0\n",
      "f11         10\n",
      "f12          0\n",
      "f13          0\n",
      "f14          0\n",
      "target       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ваш код здесь\n",
    "print(Xtrain.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7500 entries, 0 to 7499\n",
      "Data columns (total 15 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   f1      7500 non-null   int64  \n",
      " 1   f2      7425 non-null   float64\n",
      " 2   f3      7500 non-null   int64  \n",
      " 3   f4      7500 non-null   float64\n",
      " 4   f5      7500 non-null   int64  \n",
      " 5   f6      7500 non-null   float64\n",
      " 6   f7      5625 non-null   float64\n",
      " 7   f8      7500 non-null   float64\n",
      " 8   f9      7500 non-null   float64\n",
      " 9   f10     7500 non-null   float64\n",
      " 10  f11     7490 non-null   float64\n",
      " 11  f12     7500 non-null   int64  \n",
      " 12  f13     7500 non-null   int64  \n",
      " 13  f14     7500 non-null   float64\n",
      " 14  target  7500 non-null   int64  \n",
      "dtypes: float64(9), int64(6)\n",
      "memory usage: 879.0 KB\n"
     ]
    }
   ],
   "source": [
    "Xtrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              f1        f2        f3        f4        f5        f6        f7  \\\n",
      "f1      1.000000  0.125707 -0.073050  0.112205  0.027038  0.337874  0.121423   \n",
      "f2      0.125707  1.000000 -0.029289  0.140138  0.127031  0.147146  0.239829   \n",
      "f3     -0.073050 -0.029289  1.000000 -0.042141 -0.064165 -0.026528 -0.045178   \n",
      "f4      0.112205  0.140138 -0.042141  1.000000  0.890847  0.119112  0.494116   \n",
      "f5      0.027038  0.127031 -0.064165  0.890847  1.000000  0.078884  0.470290   \n",
      "f6      0.337874  0.147146 -0.026528  0.119112  0.078884  1.000000  0.176419   \n",
      "f7      0.121423  0.239829 -0.045178  0.494116  0.470290  0.176419  1.000000   \n",
      "f8      0.355870  0.155305 -0.030084  0.141998  0.101603  0.970146  0.196481   \n",
      "f9      0.028806  0.035175 -0.090260  0.090250  0.087710  0.098415  0.098406   \n",
      "f10     0.081405  0.110230  0.031323  0.045335  0.006922  0.404981  0.110439   \n",
      "f11     0.063056  0.078790 -0.001988  0.166163  0.121473  0.087553  0.104129   \n",
      "f12     0.068098  0.039166 -0.009039  0.088649  0.070077  0.091073  0.082223   \n",
      "f13     0.063043  0.191285 -0.026998  0.172236  0.144001  0.213057  0.212686   \n",
      "f14     0.060450  0.036015 -0.117053  0.118368  0.200022  0.021380  0.124722   \n",
      "target  0.232241  0.178782 -0.025223  0.355809  0.316971  0.446191  0.340129   \n",
      "\n",
      "              f8        f9       f10       f11       f12       f13       f14  \\\n",
      "f1      0.355870  0.028806  0.081405  0.063056  0.068098  0.063043  0.060450   \n",
      "f2      0.155305  0.035175  0.110230  0.078790  0.039166  0.191285  0.036015   \n",
      "f3     -0.030084 -0.090260  0.031323 -0.001988 -0.009039 -0.026998 -0.117053   \n",
      "f4      0.141998  0.090250  0.045335  0.166163  0.088649  0.172236  0.118368   \n",
      "f5      0.101603  0.087710  0.006922  0.121473  0.070077  0.144001  0.200022   \n",
      "f6      0.970146  0.098415  0.404981  0.087553  0.091073  0.213057  0.021380   \n",
      "f7      0.196481  0.098406  0.110439  0.104129  0.082223  0.212686  0.124722   \n",
      "f8      1.000000  0.103986  0.387139  0.089547  0.096546  0.227986  0.039384   \n",
      "f9      0.103986  1.000000  0.100186  0.024393  0.029808  0.050370  0.061350   \n",
      "f10     0.387139  0.100186  1.000000  0.041829  0.054964  0.225151  0.008463   \n",
      "f11     0.089547  0.024393  0.041829  1.000000 -0.031662  0.063154  0.013853   \n",
      "f12     0.096546  0.029808  0.054964 -0.031662  1.000000  0.064842  0.022723   \n",
      "f13     0.227986  0.050370  0.225151  0.063154  0.064842  1.000000  0.021525   \n",
      "f14     0.039384  0.061350  0.008463  0.013853  0.022723  0.021525  1.000000   \n",
      "target  0.453420  0.104383  0.205664  0.217820  0.162584  0.223599  0.098095   \n",
      "\n",
      "          target  \n",
      "f1      0.232241  \n",
      "f2      0.178782  \n",
      "f3     -0.025223  \n",
      "f4      0.355809  \n",
      "f5      0.316971  \n",
      "f6      0.446191  \n",
      "f7      0.340129  \n",
      "f8      0.453420  \n",
      "f9      0.104383  \n",
      "f10     0.205664  \n",
      "f11     0.217820  \n",
      "f12     0.162584  \n",
      "f13     0.223599  \n",
      "f14     0.098095  \n",
      "target  1.000000  \n"
     ]
    }
   ],
   "source": [
    "df_corr = Xtrain.select_dtypes(include='number')\n",
    "corr_matrix = df_corr.corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1        0\n",
      "f2        0\n",
      "f3        0\n",
      "f4        0\n",
      "f5        0\n",
      "f6        0\n",
      "f7        0\n",
      "f8        0\n",
      "f9        0\n",
      "f10       0\n",
      "f11       0\n",
      "f12       0\n",
      "f13       0\n",
      "f14       0\n",
      "target    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Заполняем нулевые значения.\n",
    "numerical = Xtrain.columns\n",
    "\n",
    "for feat in numerical:\n",
    "    Xtrain[feat].fillna(Xtrain[feat].median(), inplace=True)\n",
    "    \n",
    "print(Xtrain.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 15)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              f1        f2        f3        f4        f5        f6        f7  \\\n",
      "f1      1.000000  0.125035 -0.073050  0.112205  0.027038  0.337874  0.105316   \n",
      "f2      0.125035  1.000000 -0.028850  0.138873  0.125982  0.146010  0.206408   \n",
      "f3     -0.073050 -0.028850  1.000000 -0.042141 -0.064165 -0.026528 -0.039115   \n",
      "f4      0.112205  0.138873 -0.042141  1.000000  0.890847  0.119112  0.431624   \n",
      "f5      0.027038  0.125982 -0.064165  0.890847  1.000000  0.078884  0.411126   \n",
      "f6      0.337874  0.146010 -0.026528  0.119112  0.078884  1.000000  0.152859   \n",
      "f7      0.105316  0.206408 -0.039115  0.431624  0.411126  0.152859  1.000000   \n",
      "f8      0.355870  0.154120 -0.030084  0.141998  0.101603  0.970146  0.170190   \n",
      "f9      0.028806  0.035030 -0.090260  0.090250  0.087710  0.098415  0.084335   \n",
      "f10     0.081405  0.109403  0.031323  0.045335  0.006922  0.404981  0.095584   \n",
      "f11     0.062916  0.077154 -0.001944  0.165909  0.121327  0.087518  0.084696   \n",
      "f12     0.068098  0.038437 -0.009039  0.088649  0.070077  0.091073  0.072654   \n",
      "f13     0.063043  0.190207 -0.026998  0.172236  0.144001  0.213057  0.185075   \n",
      "f14     0.060450  0.035820 -0.117053  0.118368  0.200022  0.021380  0.106222   \n",
      "target  0.232241  0.177341 -0.025223  0.355809  0.316971  0.446191  0.294776   \n",
      "\n",
      "              f8        f9       f10       f11       f12       f13       f14  \\\n",
      "f1      0.355870  0.028806  0.081405  0.062916  0.068098  0.063043  0.060450   \n",
      "f2      0.154120  0.035030  0.109403  0.077154  0.038437  0.190207  0.035820   \n",
      "f3     -0.030084 -0.090260  0.031323 -0.001944 -0.009039 -0.026998 -0.117053   \n",
      "f4      0.141998  0.090250  0.045335  0.165909  0.088649  0.172236  0.118368   \n",
      "f5      0.101603  0.087710  0.006922  0.121327  0.070077  0.144001  0.200022   \n",
      "f6      0.970146  0.098415  0.404981  0.087518  0.091073  0.213057  0.021380   \n",
      "f7      0.170190  0.084335  0.095584  0.084696  0.072654  0.185075  0.106222   \n",
      "f8      1.000000  0.103986  0.387139  0.089510  0.096546  0.227986  0.039384   \n",
      "f9      0.103986  1.000000  0.100186  0.024337  0.029808  0.050370  0.061350   \n",
      "f10     0.387139  0.100186  1.000000  0.041754  0.054964  0.225151  0.008463   \n",
      "f11     0.089510  0.024337  0.041754  1.000000 -0.031618  0.063129  0.013761   \n",
      "f12     0.096546  0.029808  0.054964 -0.031618  1.000000  0.064842  0.022723   \n",
      "f13     0.227986  0.050370  0.225151  0.063129  0.064842  1.000000  0.021525   \n",
      "f14     0.039384  0.061350  0.008463  0.013761  0.022723  0.021525  1.000000   \n",
      "target  0.453420  0.104383  0.205664  0.217618  0.162584  0.223599  0.098095   \n",
      "\n",
      "          target  \n",
      "f1      0.232241  \n",
      "f2      0.177341  \n",
      "f3     -0.025223  \n",
      "f4      0.355809  \n",
      "f5      0.316971  \n",
      "f6      0.446191  \n",
      "f7      0.294776  \n",
      "f8      0.453420  \n",
      "f9      0.104383  \n",
      "f10     0.205664  \n",
      "f11     0.217618  \n",
      "f12     0.162584  \n",
      "f13     0.223599  \n",
      "f14     0.098095  \n",
      "target  1.000000  \n"
     ]
    }
   ],
   "source": [
    "df_corr = Xtrain.select_dtypes(include='number')\n",
    "corr_matrix = df_corr.corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQkpwQGHtWcr"
   },
   "source": [
    "### 3. Оцените баланс классов в задаче\n",
    "- Затем попытайтесь устно ответить на вопрос, можно ли использовать accuracy как метрику качества в задаче? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    5708\n",
       "1    1792\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ваш код здесь # Метрика accuracy плоха для не сбалансированных выборок, поэтому она не подойдет под эту задачу.\n",
    "Xtrain['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB_ReQ62tWct"
   },
   "source": [
    "### 3. Постройте baseline-модель:\n",
    "- разбейте TrainData на тренировочные (Train) и тестовые данные (Test); \n",
    "- обучите KNN, LogisticRegression и SVC с параметрами по умолчанию на тренировочных данных (Train);\n",
    "- примените модели на тестовых данных (Test) и вычислите значение метрики f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "id": "WUfRVj5ctWcu"
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "from sklearn.metrics import f1_score\n",
    "X, y = train_test_split(Xtrain, test_size=0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "ytrain = X['target']\n",
    "Xtrain = X.drop('target', axis=1)\n",
    "ytest = y['target']\n",
    "Xtest = y.drop('target', axis=1)\n",
    "\n",
    "columns = Xtrain.columns\n",
    "Xtrain = pd.DataFrame(scaler.fit_transform(Xtrain), columns = columns)\n",
    "Xtest = pd.DataFrame(scaler.fit_transform(Xtest), columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(): 0.6227897838899804\n",
      "LogisticRegression(): 0.6325486182190379\n",
      "SVC(): 0.6334745762711864\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    KNeighborsClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SVC()\n",
    "]\n",
    "\n",
    "for m in models:\n",
    "    m.fit(Xtrain, ytrain)\n",
    "    m_pred = m.predict(Xtest)\n",
    "    print(f'{m}: {f1_score(ytest, m_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYQnr8CftWcv"
   },
   "source": [
    "### 4. Улучшите модели\n",
    "Попробуйте улучшить качество обученных моделей:\n",
    "- можете подбирать гиперпараметры моделей (лучше это делать по кросс-валидации на Train, то есть с помощью использования GridSearchCV на Train);\n",
    "- можете задавать class_weights;\n",
    "- можете вручную или при помощи методов Python генерировать новые признаки и/или удалять существующие.\n",
    "\n",
    "Это самая важная и творческая часть задания. Проводите как можно больше экспериментов!\n",
    "\n",
    "Проведите минимиум три эксперимента: для каждого типа модели минимум один эксперимент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можем удалить фичи f3, f9, f14, которые очень мало влияют на целевую переменную.\n",
    "Xtrain = Xtrain.drop(['f3', 'f9', 'f14'], axis = 1)\n",
    "Xtest = Xtest.drop(['f3', 'f9', 'f14'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.132041</td>\n",
       "      <td>0.102069</td>\n",
       "      <td>0.013778</td>\n",
       "      <td>0.345515</td>\n",
       "      <td>0.110411</td>\n",
       "      <td>0.365442</td>\n",
       "      <td>0.098434</td>\n",
       "      <td>0.062775</td>\n",
       "      <td>0.068125</td>\n",
       "      <td>0.073980</td>\n",
       "      <td>-0.000681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2</th>\n",
       "      <td>0.132041</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136547</td>\n",
       "      <td>0.124397</td>\n",
       "      <td>0.154082</td>\n",
       "      <td>0.201446</td>\n",
       "      <td>0.164215</td>\n",
       "      <td>0.111048</td>\n",
       "      <td>0.079932</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>0.189365</td>\n",
       "      <td>0.050720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f4</th>\n",
       "      <td>0.102069</td>\n",
       "      <td>0.136547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890104</td>\n",
       "      <td>0.110299</td>\n",
       "      <td>0.422322</td>\n",
       "      <td>0.132275</td>\n",
       "      <td>0.044939</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.079440</td>\n",
       "      <td>0.168129</td>\n",
       "      <td>0.006381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5</th>\n",
       "      <td>0.013778</td>\n",
       "      <td>0.124397</td>\n",
       "      <td>0.890104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068391</td>\n",
       "      <td>0.400011</td>\n",
       "      <td>0.090712</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.120019</td>\n",
       "      <td>0.063478</td>\n",
       "      <td>0.142597</td>\n",
       "      <td>0.012502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f6</th>\n",
       "      <td>0.345515</td>\n",
       "      <td>0.154082</td>\n",
       "      <td>0.110299</td>\n",
       "      <td>0.068391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.158797</td>\n",
       "      <td>0.968439</td>\n",
       "      <td>0.400843</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>0.082208</td>\n",
       "      <td>0.208472</td>\n",
       "      <td>0.504716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7</th>\n",
       "      <td>0.110411</td>\n",
       "      <td>0.201446</td>\n",
       "      <td>0.422322</td>\n",
       "      <td>0.400011</td>\n",
       "      <td>0.158797</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170697</td>\n",
       "      <td>0.100963</td>\n",
       "      <td>0.080784</td>\n",
       "      <td>0.073009</td>\n",
       "      <td>0.177028</td>\n",
       "      <td>0.031762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f8</th>\n",
       "      <td>0.365442</td>\n",
       "      <td>0.164215</td>\n",
       "      <td>0.132275</td>\n",
       "      <td>0.090712</td>\n",
       "      <td>0.968439</td>\n",
       "      <td>0.170697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.387116</td>\n",
       "      <td>0.090076</td>\n",
       "      <td>0.088699</td>\n",
       "      <td>0.222058</td>\n",
       "      <td>0.562081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f10</th>\n",
       "      <td>0.098434</td>\n",
       "      <td>0.111048</td>\n",
       "      <td>0.044939</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.400843</td>\n",
       "      <td>0.100963</td>\n",
       "      <td>0.387116</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048712</td>\n",
       "      <td>0.052404</td>\n",
       "      <td>0.234475</td>\n",
       "      <td>0.217468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f11</th>\n",
       "      <td>0.062775</td>\n",
       "      <td>0.079932</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.120019</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>0.080784</td>\n",
       "      <td>0.090076</td>\n",
       "      <td>0.048712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031214</td>\n",
       "      <td>0.058576</td>\n",
       "      <td>0.037503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f12</th>\n",
       "      <td>0.068125</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>0.079440</td>\n",
       "      <td>0.063478</td>\n",
       "      <td>0.082208</td>\n",
       "      <td>0.073009</td>\n",
       "      <td>0.088699</td>\n",
       "      <td>0.052404</td>\n",
       "      <td>-0.031214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068086</td>\n",
       "      <td>0.042597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f13</th>\n",
       "      <td>0.073980</td>\n",
       "      <td>0.189365</td>\n",
       "      <td>0.168129</td>\n",
       "      <td>0.142597</td>\n",
       "      <td>0.208472</td>\n",
       "      <td>0.177028</td>\n",
       "      <td>0.222058</td>\n",
       "      <td>0.234475</td>\n",
       "      <td>0.058576</td>\n",
       "      <td>0.068086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f15</th>\n",
       "      <td>-0.000681</td>\n",
       "      <td>0.050720</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.504716</td>\n",
       "      <td>0.031762</td>\n",
       "      <td>0.562081</td>\n",
       "      <td>0.217468</td>\n",
       "      <td>0.037503</td>\n",
       "      <td>0.042597</td>\n",
       "      <td>-0.022018</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           f1        f2        f4        f5        f6        f7        f8  \\\n",
       "f1   1.000000  0.132041  0.102069  0.013778  0.345515  0.110411  0.365442   \n",
       "f2   0.132041  1.000000  0.136547  0.124397  0.154082  0.201446  0.164215   \n",
       "f4   0.102069  0.136547  1.000000  0.890104  0.110299  0.422322  0.132275   \n",
       "f5   0.013778  0.124397  0.890104  1.000000  0.068391  0.400011  0.090712   \n",
       "f6   0.345515  0.154082  0.110299  0.068391  1.000000  0.158797  0.968439   \n",
       "f7   0.110411  0.201446  0.422322  0.400011  0.158797  1.000000  0.170697   \n",
       "f8   0.365442  0.164215  0.132275  0.090712  0.968439  0.170697  1.000000   \n",
       "f10  0.098434  0.111048  0.044939  0.007514  0.400843  0.100963  0.387116   \n",
       "f11  0.062775  0.079932  0.163900  0.120019  0.089183  0.080784  0.090076   \n",
       "f12  0.068125  0.034213  0.079440  0.063478  0.082208  0.073009  0.088699   \n",
       "f13  0.073980  0.189365  0.168129  0.142597  0.208472  0.177028  0.222058   \n",
       "f15 -0.000681  0.050720  0.006381  0.012502  0.504716  0.031762  0.562081   \n",
       "\n",
       "          f10       f11       f12       f13       f15  \n",
       "f1   0.098434  0.062775  0.068125  0.073980 -0.000681  \n",
       "f2   0.111048  0.079932  0.034213  0.189365  0.050720  \n",
       "f4   0.044939  0.163900  0.079440  0.168129  0.006381  \n",
       "f5   0.007514  0.120019  0.063478  0.142597  0.012502  \n",
       "f6   0.400843  0.089183  0.082208  0.208472  0.504716  \n",
       "f7   0.100963  0.080784  0.073009  0.177028  0.031762  \n",
       "f8   0.387116  0.090076  0.088699  0.222058  0.562081  \n",
       "f10  1.000000  0.048712  0.052404  0.234475  0.217468  \n",
       "f11  0.048712  1.000000 -0.031214  0.058576  0.037503  \n",
       "f12  0.052404 -0.031214  1.000000  0.068086  0.042597  \n",
       "f13  0.234475  0.058576  0.068086  1.000000 -0.022018  \n",
       "f15  0.217468  0.037503  0.042597 -0.022018  1.000000  "
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Добавляем новую фичу : влияние 8 на 6ую. Заменяем удаленные, которые будут влиять лучше на целевую.\n",
    "Xtrain['f15'] = Xtrain.f8 * Xtrain.f6\n",
    "Xtest['f15'] = Xtest.f8 * Xtest.f6\n",
    "Xtrain.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.132041</td>\n",
       "      <td>0.102069</td>\n",
       "      <td>0.013778</td>\n",
       "      <td>0.345515</td>\n",
       "      <td>0.110411</td>\n",
       "      <td>0.365442</td>\n",
       "      <td>0.098434</td>\n",
       "      <td>0.062775</td>\n",
       "      <td>0.068125</td>\n",
       "      <td>0.073980</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>0.102069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2</th>\n",
       "      <td>0.132041</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136547</td>\n",
       "      <td>0.124397</td>\n",
       "      <td>0.154082</td>\n",
       "      <td>0.201446</td>\n",
       "      <td>0.164215</td>\n",
       "      <td>0.111048</td>\n",
       "      <td>0.079932</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>0.189365</td>\n",
       "      <td>0.050720</td>\n",
       "      <td>0.136547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f4</th>\n",
       "      <td>0.102069</td>\n",
       "      <td>0.136547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890104</td>\n",
       "      <td>0.110299</td>\n",
       "      <td>0.422322</td>\n",
       "      <td>0.132275</td>\n",
       "      <td>0.044939</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.079440</td>\n",
       "      <td>0.168129</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5</th>\n",
       "      <td>0.013778</td>\n",
       "      <td>0.124397</td>\n",
       "      <td>0.890104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068391</td>\n",
       "      <td>0.400011</td>\n",
       "      <td>0.090712</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.120019</td>\n",
       "      <td>0.063478</td>\n",
       "      <td>0.142597</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.890104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f6</th>\n",
       "      <td>0.345515</td>\n",
       "      <td>0.154082</td>\n",
       "      <td>0.110299</td>\n",
       "      <td>0.068391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.158797</td>\n",
       "      <td>0.968439</td>\n",
       "      <td>0.400843</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>0.082208</td>\n",
       "      <td>0.208472</td>\n",
       "      <td>0.504716</td>\n",
       "      <td>0.110299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7</th>\n",
       "      <td>0.110411</td>\n",
       "      <td>0.201446</td>\n",
       "      <td>0.422322</td>\n",
       "      <td>0.400011</td>\n",
       "      <td>0.158797</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170697</td>\n",
       "      <td>0.100963</td>\n",
       "      <td>0.080784</td>\n",
       "      <td>0.073009</td>\n",
       "      <td>0.177028</td>\n",
       "      <td>0.031762</td>\n",
       "      <td>0.422322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f8</th>\n",
       "      <td>0.365442</td>\n",
       "      <td>0.164215</td>\n",
       "      <td>0.132275</td>\n",
       "      <td>0.090712</td>\n",
       "      <td>0.968439</td>\n",
       "      <td>0.170697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.387116</td>\n",
       "      <td>0.090076</td>\n",
       "      <td>0.088699</td>\n",
       "      <td>0.222058</td>\n",
       "      <td>0.562081</td>\n",
       "      <td>0.132275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f10</th>\n",
       "      <td>0.098434</td>\n",
       "      <td>0.111048</td>\n",
       "      <td>0.044939</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.400843</td>\n",
       "      <td>0.100963</td>\n",
       "      <td>0.387116</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048712</td>\n",
       "      <td>0.052404</td>\n",
       "      <td>0.234475</td>\n",
       "      <td>0.217468</td>\n",
       "      <td>0.044939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f11</th>\n",
       "      <td>0.062775</td>\n",
       "      <td>0.079932</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.120019</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>0.080784</td>\n",
       "      <td>0.090076</td>\n",
       "      <td>0.048712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031214</td>\n",
       "      <td>0.058576</td>\n",
       "      <td>0.037503</td>\n",
       "      <td>0.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f12</th>\n",
       "      <td>0.068125</td>\n",
       "      <td>0.034213</td>\n",
       "      <td>0.079440</td>\n",
       "      <td>0.063478</td>\n",
       "      <td>0.082208</td>\n",
       "      <td>0.073009</td>\n",
       "      <td>0.088699</td>\n",
       "      <td>0.052404</td>\n",
       "      <td>-0.031214</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068086</td>\n",
       "      <td>0.042597</td>\n",
       "      <td>0.079440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f13</th>\n",
       "      <td>0.073980</td>\n",
       "      <td>0.189365</td>\n",
       "      <td>0.168129</td>\n",
       "      <td>0.142597</td>\n",
       "      <td>0.208472</td>\n",
       "      <td>0.177028</td>\n",
       "      <td>0.222058</td>\n",
       "      <td>0.234475</td>\n",
       "      <td>0.058576</td>\n",
       "      <td>0.068086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022018</td>\n",
       "      <td>0.168129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f15</th>\n",
       "      <td>-0.000681</td>\n",
       "      <td>0.050720</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.504716</td>\n",
       "      <td>0.031762</td>\n",
       "      <td>0.562081</td>\n",
       "      <td>0.217468</td>\n",
       "      <td>0.037503</td>\n",
       "      <td>0.042597</td>\n",
       "      <td>-0.022018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f16</th>\n",
       "      <td>0.102069</td>\n",
       "      <td>0.136547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890104</td>\n",
       "      <td>0.110299</td>\n",
       "      <td>0.422322</td>\n",
       "      <td>0.132275</td>\n",
       "      <td>0.044939</td>\n",
       "      <td>0.163900</td>\n",
       "      <td>0.079440</td>\n",
       "      <td>0.168129</td>\n",
       "      <td>0.006381</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           f1        f2        f4        f5        f6        f7        f8  \\\n",
       "f1   1.000000  0.132041  0.102069  0.013778  0.345515  0.110411  0.365442   \n",
       "f2   0.132041  1.000000  0.136547  0.124397  0.154082  0.201446  0.164215   \n",
       "f4   0.102069  0.136547  1.000000  0.890104  0.110299  0.422322  0.132275   \n",
       "f5   0.013778  0.124397  0.890104  1.000000  0.068391  0.400011  0.090712   \n",
       "f6   0.345515  0.154082  0.110299  0.068391  1.000000  0.158797  0.968439   \n",
       "f7   0.110411  0.201446  0.422322  0.400011  0.158797  1.000000  0.170697   \n",
       "f8   0.365442  0.164215  0.132275  0.090712  0.968439  0.170697  1.000000   \n",
       "f10  0.098434  0.111048  0.044939  0.007514  0.400843  0.100963  0.387116   \n",
       "f11  0.062775  0.079932  0.163900  0.120019  0.089183  0.080784  0.090076   \n",
       "f12  0.068125  0.034213  0.079440  0.063478  0.082208  0.073009  0.088699   \n",
       "f13  0.073980  0.189365  0.168129  0.142597  0.208472  0.177028  0.222058   \n",
       "f15 -0.000681  0.050720  0.006381  0.012502  0.504716  0.031762  0.562081   \n",
       "f16  0.102069  0.136547  1.000000  0.890104  0.110299  0.422322  0.132275   \n",
       "\n",
       "          f10       f11       f12       f13       f15       f16  \n",
       "f1   0.098434  0.062775  0.068125  0.073980 -0.000681  0.102069  \n",
       "f2   0.111048  0.079932  0.034213  0.189365  0.050720  0.136547  \n",
       "f4   0.044939  0.163900  0.079440  0.168129  0.006381  1.000000  \n",
       "f5   0.007514  0.120019  0.063478  0.142597  0.012502  0.890104  \n",
       "f6   0.400843  0.089183  0.082208  0.208472  0.504716  0.110299  \n",
       "f7   0.100963  0.080784  0.073009  0.177028  0.031762  0.422322  \n",
       "f8   0.387116  0.090076  0.088699  0.222058  0.562081  0.132275  \n",
       "f10  1.000000  0.048712  0.052404  0.234475  0.217468  0.044939  \n",
       "f11  0.048712  1.000000 -0.031214  0.058576  0.037503  0.163900  \n",
       "f12  0.052404 -0.031214  1.000000  0.068086  0.042597  0.079440  \n",
       "f13  0.234475  0.058576  0.068086  1.000000 -0.022018  0.168129  \n",
       "f15  0.217468  0.037503  0.042597 -0.022018  1.000000  0.006381  \n",
       "f16  0.044939  0.163900  0.079440  0.168129  0.006381  1.000000  "
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Добавляем новую фичу.\n",
    "Xtrain['f16'] = Xtrain.f4 ** 1/2\n",
    "Xtest['f16'] = Xtest.f4 ** 1/2\n",
    "Xtrain.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 2.0,\n",
       " 'class_weight': None,\n",
       " 'kernel': 'sigmoid',\n",
       " 'max_iter': 100,\n",
       " 'random_state': 42}"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Подброр настроек для модели SVC() с помощью GridSearch. Также вес классов, выставленный на баланс.\n",
    "param_grid = {\n",
    "    'C': [0.1, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0],\n",
    "    'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'max_iter': list(range(100, 500, 100)),\n",
    "    'random_state': [42]\n",
    "    }\n",
    "grid_search_svc = GridSearchCV(\n",
    "    estimator=SVC(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_micro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1)\n",
    "\n",
    "grid_search_svc.fit(Xtrain, ytrain)\n",
    "\n",
    "best_params = grid_search_svc.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "960 fits failed out of a total of 2160.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan 0.78819048 0.78952381 0.79161905 0.79047619 0.79161905\n",
      "        nan        nan        nan        nan 0.78819048 0.78952381\n",
      " 0.79161905 0.79047619 0.79161905        nan        nan        nan\n",
      "        nan 0.78819048 0.78952381 0.79161905 0.79047619 0.79161905\n",
      "        nan        nan        nan        nan 0.78819048 0.78952381\n",
      " 0.79161905 0.79047619 0.79161905        nan        nan        nan\n",
      "        nan 0.7912381  0.79219048 0.79161905 0.79142857 0.79180952\n",
      "        nan        nan        nan        nan 0.7912381  0.79219048\n",
      " 0.79161905 0.79142857 0.79180952        nan        nan        nan\n",
      "        nan 0.7912381  0.79219048 0.79161905 0.79142857 0.79180952\n",
      "        nan        nan        nan        nan 0.7912381  0.79219048\n",
      " 0.79161905 0.79142857 0.79180952        nan        nan        nan\n",
      "        nan 0.792      0.792      0.79161905 0.79161905 0.79161905\n",
      "        nan        nan        nan        nan 0.792      0.792\n",
      " 0.79161905 0.79161905 0.79161905        nan        nan        nan\n",
      "        nan 0.792      0.792      0.79161905 0.79161905 0.79161905\n",
      "        nan        nan        nan        nan 0.792      0.792\n",
      " 0.79161905 0.79161905 0.79161905        nan        nan        nan\n",
      "        nan 0.79180952 0.79180952 0.79142857 0.79142857 0.79161905\n",
      "        nan        nan        nan        nan 0.79180952 0.79180952\n",
      " 0.79142857 0.79142857 0.79142857        nan        nan        nan\n",
      "        nan 0.79180952 0.79180952 0.79142857 0.79142857 0.79142857\n",
      "        nan        nan        nan        nan 0.79180952 0.79180952\n",
      " 0.79142857 0.79142857 0.79142857        nan        nan        nan\n",
      "        nan 0.79161905 0.79180952 0.79142857 0.79142857 0.79142857\n",
      "        nan        nan        nan        nan 0.79161905 0.79161905\n",
      " 0.79142857 0.79142857 0.79142857        nan        nan        nan\n",
      "        nan 0.79161905 0.79161905 0.79142857 0.79142857 0.79142857\n",
      "        nan        nan        nan        nan 0.79161905 0.79161905\n",
      " 0.79142857 0.79142857 0.79142857        nan        nan        nan\n",
      "        nan 0.79142857 0.79142857 0.7912381  0.79142857 0.79142857\n",
      "        nan        nan        nan        nan 0.79142857 0.79142857\n",
      " 0.7912381  0.79142857 0.79142857        nan        nan        nan\n",
      "        nan 0.79142857 0.79142857 0.7912381  0.79142857 0.79142857\n",
      "        nan        nan        nan        nan 0.79142857 0.79142857\n",
      " 0.7912381  0.79142857 0.79142857        nan        nan        nan\n",
      "        nan 0.7912381  0.79142857 0.7912381  0.7912381  0.79142857\n",
      "        nan        nan        nan        nan 0.7912381  0.7912381\n",
      " 0.7912381  0.7912381  0.7912381         nan        nan        nan\n",
      "        nan 0.7912381  0.7912381  0.7912381  0.7912381  0.7912381\n",
      "        nan        nan        nan        nan 0.7912381  0.7912381\n",
      " 0.7912381  0.7912381  0.7912381         nan        nan        nan\n",
      "        nan 0.7912381  0.7912381  0.7912381  0.79104762 0.79142857\n",
      "        nan        nan        nan        nan 0.7912381  0.7912381\n",
      " 0.7912381  0.79104762 0.7912381         nan        nan        nan\n",
      "        nan 0.7912381  0.7912381  0.7912381  0.79104762 0.7912381\n",
      "        nan        nan        nan        nan 0.7912381  0.7912381\n",
      " 0.7912381  0.79104762 0.7912381         nan        nan        nan\n",
      "        nan 0.7912381  0.7912381  0.79104762 0.79104762 0.7912381\n",
      "        nan        nan        nan        nan 0.7912381  0.79104762\n",
      " 0.79104762 0.79104762 0.79104762        nan        nan        nan\n",
      "        nan 0.7912381  0.79104762 0.79104762 0.79104762 0.79104762\n",
      "        nan        nan        nan        nan 0.7912381  0.79104762\n",
      " 0.79104762 0.79104762 0.79104762        nan        nan        nan\n",
      "        nan 0.7912381  0.7912381  0.79104762 0.79104762 0.7912381\n",
      "        nan        nan        nan        nan 0.7912381  0.79104762\n",
      " 0.79104762 0.79104762 0.79104762        nan        nan        nan\n",
      "        nan 0.7912381  0.79104762 0.79104762 0.79104762 0.79104762\n",
      "        nan        nan        nan        nan 0.7912381  0.79104762\n",
      " 0.79104762 0.79104762 0.79104762        nan        nan        nan\n",
      "        nan 0.7912381  0.7912381  0.79104762 0.79104762 0.7912381\n",
      "        nan        nan        nan        nan 0.7912381  0.79104762\n",
      " 0.79104762 0.79104762 0.79104762        nan        nan        nan\n",
      "        nan 0.7912381  0.79104762 0.79104762 0.79104762 0.79104762\n",
      "        nan        nan        nan        nan 0.7912381  0.79104762\n",
      " 0.79104762 0.79104762 0.79104762        nan        nan        nan\n",
      "        nan 0.7912381  0.7912381  0.79104762 0.79104762 0.7912381\n",
      "        nan        nan        nan        nan 0.7912381  0.79104762\n",
      " 0.79104762 0.79104762 0.79104762        nan        nan        nan\n",
      "        nan 0.7912381  0.79104762 0.79104762 0.79104762 0.79104762\n",
      "        nan        nan        nan        nan 0.7912381  0.79104762\n",
      " 0.79104762 0.79104762 0.79104762        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.5,\n",
       " 'class_weight': 'balanced',\n",
       " 'max_iter': 100,\n",
       " 'penalty': 'l1',\n",
       " 'random_state': 42,\n",
       " 'solver': 'saga'}"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'C': [0.1, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0],\n",
    "    'class_weight': ['balanced'],\n",
    "    'solver' : ['lbfgs', 'liblinear', 'saga'],\n",
    "    'max_iter': list(range(100, 500, 100)),\n",
    "    'random_state': [42]\n",
    "    }\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_micro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1)\n",
    "\n",
    "grid_search_lr.fit(Xtrain, ytrain)\n",
    "\n",
    "best_params = grid_search_lr.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 378 candidates, totalling 1890 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'ball_tree',\n",
       " 'leaf_size': 30,\n",
       " 'n_neighbors': 20,\n",
       " 'weights': 'uniform'}"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': list(range(5, 50, 5)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': list(range(30, 100, 10))\n",
    "    }\n",
    "grid_search_knn = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_micro',\n",
    "    verbose=1,\n",
    "    n_jobs=-1)\n",
    "\n",
    "grid_search_knn.fit(Xtrain, ytrain)\n",
    "\n",
    "best_params = grid_search_knn.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6203319502074689"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(algorithm='ball_tree', leaf_size=45, n_neighbors=35, weights='distance')\n",
    "knn.fit(Xtrain,ytrain)\n",
    "knn_pred = knn.predict(Xtest)\n",
    "f1_score(ytest, knn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6834895457822638"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.1, class_weight='balanced', max_iter=100, penalty='l1', random_state=42, solver='saga')\n",
    "lr.fit(Xtrain,ytrain)\n",
    "lr_pred = lr.predict(Xtest)\n",
    "f1_score(ytest, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6311166875784191"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(C=8.0, class_weight='balanced', kernel='sigmoid', max_iter=200, random_state=42)\n",
    "svm.fit(Xtrain,ytrain)\n",
    "svm_pred = svm.predict(Xtest)\n",
    "f1_score(ytest, svm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfu3yy_7tWcy"
   },
   "source": [
    "### 5. Оцените на отложенной выборке качество наилучшей модели\n",
    "В пунктах 3 и 4 вы построили много разных моделей.\n",
    "\n",
    "Возьмите ту, которая дала наилучшее качество на тестовых данных (Test). Примените её на отложенной выборке (TestData) и выведите на экран значение метрики f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "id": "upHUOfrktWcy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3803045027534823"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ваш код здесь\n",
    "X_TEST = TEST.drop(['target','f14','f3','f9'], axis = 1)\n",
    "X_TEST['f15'] = X_TEST.f8 * X_TEST.f6\n",
    "X_TEST['f16'] = X_TEST.f4 ** 1/2\n",
    "y_TEST = TEST.target\n",
    "lr_pred = lr.predict(X_TEST)\n",
    "f1_score(y_TEST, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3803045027534823"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pred = knn.predict(X_TEST)\n",
    "f1_score(y_TEST, knn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Значит ли это, что модель переобучилась?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZT0rMuStWcz"
   },
   "source": [
    "### 6. Выполните хитрый трюк\n",
    "Часто смешивание различных моделей даёт улучшение итогового предсказания. Попробуйте смешать две лучшие модели по формуле:\n",
    "$$pred_{final} = \\alpha\\cdot pred_1 + (1-\\alpha)\\cdot pred_2$$.\n",
    "\n",
    "Значение $\\alpha$ подберите в цикле по Test-выборке. Оцените качество на отложенной выборке.\n",
    "\n",
    "Удалось ли добиться улучшения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "id": "3DdSsk6gtWc7",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3803045027534823"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ваш код здесь\n",
    "new_pred = []\n",
    "lr_Pred = lr.predict(X_TEST)\n",
    "knn_Pred = knn.predict(X_TEST)\n",
    "\n",
    "for y in range(2500):\n",
    "    new_pred.append(y_TEST[y] * lr_Pred[y] + (1 - y_TEST[y] * knn_Pred[y]))\n",
    "    \n",
    "f1_score(y_TEST, new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b9imFkLtWc8"
   },
   "source": [
    "### 7. Сделайте выводы\n",
    "\n",
    "Запишите в отдельной ячейке текстом выводы о проделанной работе. Для этого ответьте на вопросы:\n",
    "- Какие подходы вы использовали для улучшения работы baseline-моделей?\n",
    "- Какого максимального качества удалось добиться на Test-данных?\n",
    "- Какое при этом получилось качество на отложенной выборке? \n",
    "- Ваша модель переобучилась, недообучилась или обучилась как надо?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWgT4ulytWc-"
   },
   "source": [
    "# Выводы\n",
    "1) Для улучшения моделей были убраны все NaN, точнее заполнены median.\n",
    "В некоторых моделях была использована регуляризация.\n",
    "В некоторых моделях был изменен вес классов.\n",
    "Из выборок были удалены некоторые маловлияющие признаки и добавлены новые.\n",
    "\n",
    "2) Произошло переобучение моделей и модель начала выявлять только один класс - 1. 38%\n",
    "\n",
    "3) С помощью линейной регрессии смог поднять f1_score модели до 68%\n",
    "\n",
    "4) Моя модель переобучилась."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnFjm55atWc-"
   },
   "source": [
    "Важный комментарий! В реальных задачах не следует ожидать, что машинным обучением всегда удастся решить задачу с хорошим качеством. Но использовать все имеющиеся у вас в арсенале методы для достижения наилучшего результата нужно."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
